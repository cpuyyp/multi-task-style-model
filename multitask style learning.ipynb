{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785f4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "# from torch.utils.data.sampler import RandomSampler\n",
    "import numpy as np\n",
    "import collections\n",
    "import json\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import AutoConfig, AutoTokenizer, BertModel, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc31b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8839076",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tasks.json', 'r') as f:\n",
    "    tasks = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce7ff39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CrowdFlower': 13,\n",
       " 'DailyDialog': 7,\n",
       " 'EmoBank_Valence': 1,\n",
       " 'EmoBank_Arousal': 1,\n",
       " 'EmoBank_Dominance': 1,\n",
       " 'HateOffensive': 3,\n",
       " 'PASTEL_age': 8,\n",
       " 'PASTEL_country': 2,\n",
       " 'PASTEL_education': 10,\n",
       " 'PASTEL_ethnic': 10,\n",
       " 'PASTEL_gender': 3,\n",
       " 'PASTEL_politics': 3,\n",
       " 'PASTEL_tod': 5,\n",
       " 'SARC': 2,\n",
       " 'SarcasmGhosh': 2,\n",
       " 'SentiTreeBank': 1,\n",
       " 'ShortHumor': 2,\n",
       " 'ShortJokeKaggle': 2,\n",
       " 'ShortRomance': 2,\n",
       " 'StanfordPoliteness': 1,\n",
       " 'TroFi': 2,\n",
       " 'VUA': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc05744",
   "metadata": {},
   "source": [
    "# multi-task dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00309940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    # currently it's a Mapping-style dataset. Not sure if a Iterable-style dataset will be better\n",
    "    def __init__(self, tsv_file):\n",
    "        self.df = pd.read_csv(tsv_file, sep='\\t')\n",
    "        if self.df['label'].dtype == 'float64':\n",
    "            self.df['label'] = self.df['label'].astype('float32')\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        dataslice = self.df.iloc[idx]\n",
    "        sample = {'text':dataslice['text'], 'label':dataslice['label']}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6463e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainDataLoader():\n",
    "    '''\n",
    "    Each time, a random integer selects a dataset and load a batch of data {text, label} from it. Return i_task and data\n",
    "    \n",
    "    Known issue: large dataset may have not iterate once, small datasets may have been iterated many times\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, tasks, batch_size, shuffle, num_workers):\n",
    "        self.tasks = tasks\n",
    "        self.split = 'train'\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in tasks:\n",
    "            self.datasets.append(MyDataset('./processed/'+self.split+'/'+task+'.tsv'))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers)) \n",
    "            self.dataloaderiters.append(iter(self.dataloaders[-1]))\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        i_task = np.random.randint(self.num_tasks)\n",
    "        if self.n < self.len:\n",
    "            self.n += 1\n",
    "        else:\n",
    "            raise StopIteration\n",
    "            \n",
    "        try: \n",
    "            dataloaderiter = self.dataloaderiters[i_task]\n",
    "            batch = next(dataloaderiter)\n",
    "        except StopIteration:\n",
    "            self.dataloaderiters[i_task] = iter(self.dataloaders[i_task])\n",
    "            dataloaderiter = self.dataloaderiters[i_task]\n",
    "            batch = next(dataloaderiter)\n",
    "        return i_task, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850c7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTestDataLoader():\n",
    "    '''\n",
    "    TODO\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, tasks, split, batch_size, shuffle, num_workers):\n",
    "        assert split in ['dev', 'test'], 'not implemented'\n",
    "        self.tasks = tasks\n",
    "        self.split = split\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.num_tasks = len(tasks)\n",
    "        self.datasets = []\n",
    "        self.dataloaders = []\n",
    "        self.dataloaderiters = []\n",
    "        self.len = 0\n",
    "        for task in tasks:\n",
    "            self.datasets.append(MyDataset('./processed/'+self.split+'/'+task+'.tsv'))\n",
    "            self.dataloaders.append(DataLoader(self.datasets[-1], batch_size=self.batch_size, shuffle=self.shuffle, num_workers=self.num_workers)) \n",
    "            self.dataloaderiters.append(iter(self.dataloaders[-1]))\n",
    "            self.len += len(self.dataloaders[-1])\n",
    "    def __len__(self):   \n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        i_task = np.random.randint(self.num_tasks)\n",
    "        if self.n < self.len:\n",
    "            self.n += 1\n",
    "        else:\n",
    "            raise StopIteration\n",
    "            \n",
    "        try: \n",
    "            dataloaderiter = self.dataloaderiters[i_task]\n",
    "            batch = next(dataloaderiter)\n",
    "        except StopIteration:\n",
    "            self.dataloaderiters[i_task] = iter(self.dataloaders[i_task])\n",
    "            dataloaderiter = self.dataloaderiters[i_task]\n",
    "            batch = next(dataloaderiter)\n",
    "        return i_task, batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d3b8d",
   "metadata": {},
   "source": [
    "# multi-task model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9c05444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a55d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden1(sent_emb)\n",
    "        output = self.hidden2(output).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_fn(output, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69dc3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, embedding_dim = 768, hidden_dim = 128):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, num_labels)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    def forward(self, sent_emb, label):\n",
    "        output = self.hidden1(sent_emb)\n",
    "        output = self.hidden2(output)\n",
    "        output = self.activation(output)\n",
    "        \n",
    "        loss = self.loss_fn(output, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eec4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBert(BertPreTrainedModel):\n",
    "    def __init__(self, config, tasks, use_pooler=True):\n",
    "        super().__init__(config)\n",
    "        self.use_pooler = use_pooler\n",
    "        self.basemodel = BertModel(config)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        for task in tasks:\n",
    "            if tasks[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks[task]))\n",
    "    def forward(self, i_task, input_ids, token_type_ids, attention_mask, label):\n",
    "        output = self.basemodel(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        if self.use_pooler:\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        head = self.style_heads[i_task]\n",
    "        output, loss = head(sent_emb, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f8ebd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskRoberta(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, tasks, use_pooler=True):\n",
    "        super().__init__(config)\n",
    "        self.use_pooler = use_pooler\n",
    "        self.basemodel = RobertaModel(config)\n",
    "        self.style_heads = nn.ModuleList()\n",
    "        for task in tasks:\n",
    "            if tasks[task] == 1:\n",
    "                self.style_heads.append(RegressionHead())\n",
    "            else:\n",
    "                self.style_heads.append(ClassificationHead(tasks[task]))\n",
    "    def forward(self, i_task, input_ids, attention_mask, label):\n",
    "        output = self.basemodel(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if self.use_pooler:\n",
    "            sent_emb = output['pooler_output']\n",
    "        else:\n",
    "            sent_emb = output['last_hidden_state'][:,0,:]\n",
    "        head = self.style_heads[i_task]\n",
    "        output, loss = head(sent_emb, label)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26c5eb",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e1fda3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size 32 will lead to memory issue\n",
    "mt_dataloader = MultiTaskTrainDataLoader(tasks, batch_size = 16, shuffle = True, num_workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e4d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"bert-base-uncased\"\n",
    "# base_model = 'roberta-base'\n",
    "\n",
    "config = AutoConfig.from_pretrained(base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "mt_model = MultiTaskBert(config, tasks).to(device)\n",
    "# mt_model = MultiTaskRoberta(config, tasks).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab7ab2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(mt_model.parameters(), lr=0.03)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.05, total_steps=len(mt_dataloader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19b2ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss(losses):\n",
    "    for k in losses:\n",
    "        print(f'{losses[k][-1]:4.2f}', end=' ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0768d5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bf25dcb02c46ea83b5694b767eb1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85054 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = collections.defaultdict(list)\n",
    "for i_iter, data in enumerate(tqdm(mt_dataloader)):    \n",
    "    i_task, batch = data\n",
    "    optimizer.zero_grad()\n",
    "    label = batch['label'].to(device)\n",
    "    del batch['label']\n",
    "    tokens = tokenizer(**batch, return_tensors='pt', padding=True, truncation=True, max_length=256).to(device)\n",
    "    output, loss = mt_model(**tokens, i_task=i_task,  label=label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    losses[i_task].append(loss.item())\n",
    "    if i_iter%1000 == 0 and i_iter != 0:\n",
    "        print(f'##### iter {i_iter}/{len(mt_dataloader)}')\n",
    "        print_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './mt_model_runs/mt_1.bin'\n",
    "torch.save(mt_model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
